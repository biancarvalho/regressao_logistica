\documentclass[12pt]{article}

\usepackage{setspace}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage[pdftex,bookmarks=true,bookmarksopen=false,bookmarksnumbered=true,colorlinks=true,linkcolor=black]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{pdfpages}
\usepackage[brazil]{babel}
\usepackage{helvet}
\usepackage{listings}
\usepackage{color}
\usepackage{graphicx}
 
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset{
  language=Python,                
  basicstyle=\footnotesize,           
  numbers=left,                   
  numberstyle=\tiny\color{gray},  
  stepnumber=2,                             
  numbersep=5pt,                  
  backgroundcolor=\color{white},    
  showspaces=false,               
  showstringspaces=false,         
  showtabs=false,                 
  frame=single,                   
  rulecolor=\color{black},        
  tabsize=2,                      
  captionpos=b,                   
  breaklines=true,                
  breakatwhitespace=false,        
  title=\lstname,                               
  keywordstyle=\color{blue},          
  commentstyle=\color{dkgreen},       
  stringstyle=\color{mauve},     
}

\renewcommand{\familydefault}{\sfdefault}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{assumption}{Assumption}
\newtheorem{acknowledgment}{Acknowledgment}
\newtheorem{algorithm}{Algorithm}
\newtheorem{axiom}{Axiom}
\newtheorem{case}{Case}
\newtheorem{claim}{Claim}
\newtheorem{conclusion}{Conclusion} 
\newtheorem{condition}{Condition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}[section]
\newtheorem{criterion}{Criterion}
\newtheorem{defn}{Definition}[section]

\newtheorem{example}{Example}[section]
\newtheorem{exercise}{Exercise}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{notation}{Notation}
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{remark}{Remark}
\newtheorem{solution}{Solution}
\newtheorem{summary}{Summary}
\newenvironment{proof}[1][Proof]{\textbf{#1.} }{\rule{0.5em}{0.5em}}

\begin{document}

\begin{titlepage}
\begin{center}
\textbf{\LARGE Fundação Getulio Vargas}\\ 
\textbf{\LARGE Escola de Matemática Aplicada}\\
\textbf{\LARGE Curso de Graduação em Ciência de Dados}

\par
\vspace{170pt}
\textbf{\Large Regressão Logística}\\
\vspace{80pt}
\textbf{\Large Bianca Dias de Carvalho\\ Luis Fernando Laguardia}\\
\end{center}

\par
\vfill
\begin{center}
{{\normalsize Rio de Janeiro - Brasil}\\
{\normalsize \the\year}}
\end{center}
\end{titlepage}

\thispagestyle{empty}

\newpage
\begin{center}
\textbf{\LARGE Fundação Getulio Vargas}\\ 
\textbf{\LARGE Escola de Matemática Aplicada}\\
\textbf{\LARGE Curso de Graduação em Ciência de Dados}

\par
\vspace{100pt}
\textbf{\Large Regressão Logística}


\par
\vspace{150pt}

\end{center}

\par
\vspace{65pt}
\begin{center}


\hrulefill

\vspace{5pt}
\textbf{\Large Bianca Dias de Carvalho\\ Luis Fernando Laguardia}
\end{center}

\par
\vfill
\begin{center}
{{\normalsize Rio de Janeiro - Brasil}\\
{\normalsize \the\year}}
\end{center}

\thispagestyle{empty}

\newpage
\tableofcontents
\thispagestyle{empty}

\newpage
\section{Introdução}
\hspace{0.4cm} Este trabalho aborda a regressão logística, contendo notebooks em python, onde a regressão logística é aplicada em diversas bases de dados, com o fito de realizar previsões a partir de variáveis categóricas.

A regressão logística é uma técnica de mineração de dados, que consiste no processo de encontrar padrões e correlações em grandes conjuntos de dados para prever resultados, além disso, através dela também é possível obter a probabilidade de ocorrência de cada evento, assim como a influência de cada variável independente. 

A principal diferença entre a regressão logística e a regressão linear é que a variável dependente/resposta, atributo que se quer prever, é categórica, frequentemente binária. 

\newpage
\section{Desenvolvimento}
\hspace{0.4cm} Inicialmente, foi criado uma classe de regressão logística em python.
\\
\begin{lstlisting}
class LogisticRegression:
    def __init__(self, learning_rate=0.001, n_iters=1000):
        self.lr = learning_rate
        self.n_iters = n_iters
        self.weights = None
        self.bias = None

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features) #parametro inicial
        self.bias = 0 #parametro inicial

        #gradiente descendente
        for _ in range(self.n_iters):
            linear_model = np.dot(X, self.weights) + self.bias #aproxima y com a combinacao linear dos pesos e x somada a constante
            y_predicted = self._sigmoid(linear_model) #aplica a funcao sigmoide

            #computa os gradientes
            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))
            db = (1 / n_samples) * np.sum(y_predicted - y)
            #atualiza os parametros
            self.weights -= self.lr * dw
            self.bias -= self.lr * db

    def predict(self, X):
        linear_model = np.dot(X, self.weights) + self.bias
        y_predicted = self._sigmoid(linear_model)
        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]
        
        return np.array(y_predicted_cls)

    def _sigmoid(self, x):
        return 1 / (1 + np.exp(-x))
\end{lstlisting}

O gradiente descendente é um algoritmo de otimização, seu objetivo é minimizar algumas funções movendo-se iterativamente na direção de descida mais íngreme. O parâmetro \textit{learning rate} nos diz qual distância será percorrida em cada iteração, ele não pode ser muito pequeno devido ao custo computacional, nem muito grande por falhar em convergir no mínimo local. A função \textit{sigmoide} é aplicada à soma ponderada, essa função transforma varia de 0 a 1 e tem um formato S (não é linear) e basicamente tenta empurrar os valores de y para os extremos.
\\
\begin{lstlisting}
def plot(self, X, y, legend):
        # essa funcao plota o resultado apenas se X se referir a exatamente 2 variaveis
        if X.shape[1] != 2:
            raise ValueError("Can plot only for X's that refers to exactly 2 vars.")
        
        slope = -(self.weights[0]/self.weights[1])
        intercept = -(self.bias/self.weights[1])
        predictions = self.predict(X)

        sns.set_style('white')
        sns.scatterplot(x = X[:,0], y= X[:,1], hue=y.reshape(-1), style=predictions.reshape(-1));

        ax = plt.gca()
        ax.autoscale(False)
        x_vals = np.array(ax.get_xlim())
        y_vals = intercept + (slope * x_vals)
        plt.plot(x_vals, y_vals, c="k");
        
        plt.xlabel(legend[0])
        plt.ylabel(legend[1])
\end{lstlisting}

A função acima plota o resultado da regressão linear em um gráfico de dispersão onde nos eixos estão as variáveis independentes.

\vspace{100pt}

Após isso, as bases foram importadas e os dados foram normalizados. Como exemplo, será mostrado a importação e normalização de uma das bases.
\\
\begin{lstlisting}
df = pd.read_csv("db_estrelas.csv")

df = df[(df['Spectral Class'] == 'B') | (df['Spectral Class'] == 'M')]
df['Spectral Class'].replace(to_replace='B', value=1, inplace=True)
df['Spectral Class'].replace(to_replace='M', value=0, inplace=True)

# Seleção de Dados
dados = {
    'X' : ['Temperature (K)', 'Absolute magnitude(Mv)'],
    'y' : 'Spectral Class',
    'normalizada' : False
}

df = df[ dados['X']+[dados['y']] ]
df = df.dropna()

if not dados['normalizada']:
    for col in dados['X']:
        df[[col]] = df[[col]]/df[[col]].mean()

X = df[ dados['X'] ].to_numpy()
y = df[[ dados['y'] ]].to_numpy()
y = np.hstack((y)).T

df.sample(5)
\end{lstlisting}

\vspace{120pt}
Também foi criada uma tabela que mostra os pesos de cada variável independente.\\

\begin{lstlisting}
norma_pesos = pd.DataFrame(regressor.weights)/pd.DataFrame(regressor.weights).abs().sum()
norma_pesos = norma_pesos[0].values.tolist()

dfpesos = pd.DataFrame({'Pesos':norma_pesos}, index=dados['X'])

dfpesos
\end{lstlisting}

\begin{figure}[h]
\caption{Pesos de cada variável}
\centering
\includegraphics[width=10cm]{out2.png}
\label{figura:out2}
\end{figure}

Nessa base, aplicamos a regressão logística para tentar prever a classe espectral das estrelas. Sabendo que a Classe Espectral de uma estrela é principalmente baseada em sua temperatura, mas geralmente mostrada em um gráfico de correlação temperatura x magnitude absoluta, utilizamos essas duas propriedades numéricas como colunas para a análise da regressão logística.

\hspace{0.4cm} No primeiro caso, comparando as Classes Espectrais B (muito quente) e M (muito “fria”), conseguimos atingir 100\% de precisão na aplicação do algoritmo. Porém, algo estranho de se observar é que as estrelas de Classe B estão cobrindo uma região muito dispersa de temperatura, algo incomum segundo as informações na Wikipedia.\\

\begin{lstlisting}
df = df[(df['Spectral Class'] == 'A') | (df['Spectral Class'] == 'F')]
df['Spectral Class'].replace(to_replace='A', value=1, inplace=True)
df['Spectral Class'].replace(to_replace='F', value=0, inplace=True)

dados = {'X' : ['Temperature (K)', 'Absolute magnitude(Mv)'],
    'y' : 'Spectral Class',
    'normalizada' : False}

df = df[ dados['X']+[dados['y']] ]
df = df.dropna()

if not dados['normalizada']:
    for col in dados['X']:
        df[[col]] = df[[col]]/df[[col]].mean()

X = df[ dados['X'] ].to_numpy()
y = df[[ dados['y'] ]].to_numpy()
y = np.hstack((y)).T

df.sample(5)
\end{lstlisting}

\begin{figure}[h]
\caption{Dados selecionados}
\centering
\includegraphics[width=10cm]{out1.png}
\label{figura:out1}
\end{figure}

Após isso, a regressão logística foi aplicada e a precisão da previsão foi avaliada. A precisão desse modelo em específico foi de 100\%.
\\
\begin{lstlisting}
regressor = LogisticRegression(learning_rate=0.000001, n_iters=2000)
regressor.fit(X, y)
predictions = regressor.predict(X)

def accuracy(y_true, y_pred):
    accuracy = np.sum(y_true == y_pred) / len(y_true)
    return accuracy

print(f"A precisão do modelo é: {accuracy(y, predictions)}")

norma_pesos = pd.DataFrame(regressor.weights)/pd.DataFrame(regressor.weights).abs().sum()
norma_pesos = norma_pesos[0].values.tolist()

dfpesos = pd.DataFrame({'Pesos':norma_pesos}, index=dados['X'])

dfpesos
\end{lstlisting}

Por fim, foi feita uma visualização dessa regressão.
\\
\begin{lstlisting}
try:
    regressor.plot(X, y, dados['X'])
except:
    print("Sem visualização disponível.")
\end{lstlisting}

\begin{figure}[h]
\caption{Regressão linear - Resultado Final}
\centering
\includegraphics[width=10cm]{teste.png}
\label{figura:regressao_linear}
\end{figure}

\hspace{0.4cm}No segundo caso, percebemos o que acontece. Nessa base, apenas as estrelas de classe M e as estrelas situadas na chamada Faixa Principal (região circulada) estão corretamente classificadas, enquanto as outras estão seguindo outro padrão. Por isso, ao aplicarmos a regressão logística em duas classes estelares diferentes de M e que saem da Faixa Principal (nesse caso, A e F), o algoritmo encontra outro padrão (fisicamente incorreto) e dá mais peso para a Magnitude Absoluta.

\begin{figure}[h]
\caption{Estrelas - Segundo Caso}
\centering
\includegraphics[width=10cm]{estrelas.png}
\label{figura:estrelas}
\end{figure}

Como as classes A e F estão muito próximas em temperatura, mesmo essa pequena inclinação em relação a temperatura significa um peso relativamente grande (30\%), mas nem por isso a Magnitude Absoluta deixa de ser mais importante (absurdos 70\%). 

\begin{figure}[h]
\caption{Pesos Variáveis Independentes - Segundo Caso}
\centering
\includegraphics[width=10cm]{out6.png}
\label{figura:out6}
\end{figure}

\hspace{0.4cm}Apesar da inadequação física, esse fica sendo um ótimo exemplo de como o algoritmo funciona (além de um grande aprendizado sobre averiguar as bases antes de iniciar as análises).


\clearpage
\hspace{0.4cm}A partir disso, segue o resultado final obtido com as demais bases. 
\\

\begin{itemize}
    \item \textbf{Estudantes}\\
    
    \hspace{0.4cm}Neste exemplo, a nota do pré-teste foi utilizada para prever se a nota do pós-teste será maior ou igual a 7. A precisão deste modelo foi de 89\% e conseguimos ver que essa variável independente tem grande influência na nota do pós-teste. A partir dessa análise, foi possível concluir que alunos com a nota igual ou superior à 58 no pré-teste teriam uma nota igual ou superior à 7 no pós-teste.
    
    
    \begin{figure}[h]
    \caption{Gráfico sobre os estudantes}
    \centering
    \includegraphics[width=10cm]{estudantes.jpg}
    \label{figura:estudantes}
    \end{figure}
    
    \begin{figure}[h]
    \caption{Resultado final - estudantes}
    \centering
    \includegraphics[width=10cm]{out4.png}
    \label{figura:out3}
    \end{figure}
    
    \vspace{50px}
    
    \item \textbf{Vinhos}
    
    \hspace{0.4cm}Nem toda aplicação de regressão logística pode ser perfeitamente transformada em um exemplo visual bidimensional. Nesta análise, por exemplo, de previsão de avaliação de vinhos, isso foi impossível. Isso porque o algoritmo não dava resultados suficientemente precisos utilizando apenas 2 variáveis numéricas, então utilizamos 7 das colunas disponíveis para tentar prever quais vinhos receberam nota superior a 6 (nota considerada muito boa - sommeliers são muito mesquinhos com notas).

    \hspace{0.4cm}Se utilizando 2 variáveis nosso ápice de precisão foi ~54\%, utilizando essas 7 variáveis escolhidas a dedo conseguimos atingir 74.6\% de precisão. A desvantagem nesse caso é que, por usarmos 7 variáveis, não é possível mostrar um belo gráfico em sete dimensões que mostre a melhor decisão da regressão logística, então só podemos nos contentar com a exibição dos pesos escolhidos para cada coluna. Dessa tabela, é interessante perceber, por exemplo, o quanto a quantidade de álcool está positivamente relacionada com a nota.

    \begin{figure}[h]
    \caption{Resultado final - vinhos}
    \centering
    \includegraphics[width=10cm]{out3.png}
    \label{figura:out4}
    \end{figure}
    
    \hspace{0.4cm}Além disso, é importante notar que, embora o resultado usando 7 variáveis tenha sido muito mais preciso que o resultado usando 2, nem sempre aumentar o número de variáveis significa um aumento de precisão. Nesse mesmo exemplo, se usássemos todas as 11 colunas numéricas disponíveis na base, nosso ápice de precisão seria um pouco menor (~73\%). Isso acontecia porque as outras variáveis, como, por exemplo, “chlorides”, tinham correlação real nula com a nota dos vinhos, mas, como o algoritmo sempre busca encontrar um padrão, essa variável recebia um peso (ainda que pequeno) que prejudicava a qualidade da previsão.
	
	\hspace{0.4cm}Diante dessa situação, nosso método foi utilizar inicialmente todas as colunas na regressão logística e depois selecionar apenas as que tinham um peso de 3\% ou mais em relação ao total.

    
    \item \textbf{Câncer de mama}
    
    \hspace{0.4cm}Este exemplo é muito famoso no campo da regressão logística, devido a isso, essa base já possui uma \textit{learning rate} ótima conhecida e se tornou substancialmente previsível. Além disso, é um ótimo retrato da importância dos modelos de predição para a sociedade.
    
    \hspace{0.4cm}Os dados dessa base foram calculados a partir de uma imagem digitalizada de uma massa mamária e descrevem as características dos núcleos celulares presentes na imagem. É importante ressaltar que apesar de nenhuma característica por si só ter uma influência excepcional no resultado final, algumas podem ser destacadas, como as propriedades da área e do perímetro dos núcleos. A precisão desse modelo é de 90\%. 
    
    \begin{figure}[h]
    \caption{Resultado final - câncer de mama}
    \centering
    \includegraphics[width=10cm]{out5.jpeg}
    \label{figura:out5}
    \end{figure}
\end{itemize}

\clearpage
\section{Considerações Finais}

\hspace{0.4cm}Este trabalho se propôs, como objetivo geral, mostrar aplicações da regressão logística através de diferentes bases de dados. Desta forma, é possível concluir que é possível utilizar essa técnica em inúmeras áreas, onde ela sempre é destacada como uma importante ferramenta de análise de dados. 

\newpage
\section{Referências}

[1] a.\href{https://towardsdatascience.com/logistic-regression-from-scratch-with-numpy-da4cc3121ece}{ Towards Data Science - Logistic Regression from Scratch with NumPy}

\noindent [2] b.\href{https://monografias.ufma.br/jspui/bitstream/123456789/3572/1/LEANDRO-GONZALEZ.pdf}{ Leando Gonzales - Regressão Logística e suas aplicações}

\end{document}